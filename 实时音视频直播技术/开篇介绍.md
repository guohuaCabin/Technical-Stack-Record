整体流程图：

![实时音视频直播整体流程图](http://blog.guohuaden.com/%E5%AE%9E%E6%97%B6%E9%9F%B3%E8%A7%86%E9%A2%91%E7%9B%B4%E6%92%AD%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B%E5%9B%BE.png)



实时音视频直播的一个流程如下：

音视频采集 --> 音视频处理  --> 音视频编码  --> 推流和传输  --> 音视频转码  --> 解码和渲染



* 音视频采集

  采集是音视频播放中的第一环，大多数使用的是PC端开源的OBS

* 音视频处理

  通常情况下在采集后会进行音视频的处理，例如：美颜、滤镜、水印等，「80% 的主播没有美颜根本没法看」。目前iOS比较出名的是GPUImage库，提供了丰富端预处理效果，还可以基于这个库自己写算法实现更丰富端效果。Android 也有 GPUImage 这个库的移植，叫做 android-gpuimage。同时，Google 官方开源了一个伟大的库，覆盖了 Android 上面很多多媒体和图形图像相关的处理。

* 音视频编码

  编码的难点：

  * 处理硬件兼容性问题
  * 在搞fps、低bitrate和音质画面之前找到平衡

  iOS端硬件兼容性较好，可以直接采用硬编，而安卓的硬编的至少吃则难得多，需要支持各种硬件机型，推荐使用软编。

* 推流和传输

  **传输设计到很多端：**

  * 从主播端到服务端
  * 从收流服务端到边缘节点
  * 从边缘节点到观众端

  推流端和分发端理论上需要支持的并发用户数应该都是亿级的，不过毕竟产生内容的推流端在少数，和消费内容端播放端不是一个量级，但是他们对推流稳定性和速度的要求比播放端高很多，这涉及到所有播放端能否看到直播，以及直播端质量如何。

* 音视频转码

  为了让音视频流适配各个平台端各种不同协议，需要在服务端做一些流处理工作，比如转码成不同格式支持不同协议如：RTMP、HLS和FLV，一路转多路流来适配各种不同的网络状况和不同分辨率的终端设备。同时 也可以做一些内容识别功能，如：鉴黄

* 解码和渲染

  解码和渲染 也就是播放，目前iOS端的播放兼容性较好，在延迟课接收的情况下使用HLS协议式最好的选择，也有RTMP和HLS供其选择，Android的硬件解码和编码一样也存在兼容性问题，目前比较好的开源播放器是基于ffplay的ijplayer。